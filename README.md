# ğŸ›¡ï¸Network Security - Phishing URL Detection Pipeline (End-to-End ML & MLOps)
This project demonstrates a complete **Machine Learning + MLOps workflow** for detecting phishing URLs. It covers **data ingestion â†’ validation â†’ transformation â†’ model training â†’ evaluation â†’ deployment**, with **MLflow tracking**, **Docker containerization**, **CI/CD** via GitHub Actions, and deployment on **AWS (EC2 + ECR + S3)**.

---

## âœ… Key Features

- Structured, production-style **pipeline** with **logging**, **exceptions**, and **artifacts**
- **ETL Pipeline**: Extract data from **MongoDB**, transform it **(imputation, preprocessing, feature engineering)**, and load it into the pipeline for model training
- **FastAPI** service for training and prediction (/train, /predict)
- **MLflow** integration for experiments (parameters, metrics, and artifacts)
- **Containerized with Docker** for easy deployment
- **AWS deployment** workflow: **EC2**, **ECR**, and **S3 storage**
- GitHub Actions workflows for **CI/CD** included
- Utilities to sync artifacts to/from **S3**

---

## ğŸ“¦ Project Structure

```
Network-Security/    
â”‚â”€â”€ app.py                  # FastAPI app: /train and /predict    
â”‚â”€â”€ main.py                 # Local CLI training entry (stage-by-stage)    
â”‚â”€â”€ push_data.py            # Ingest source data into MongoDB    
â”‚â”€â”€ setup.py                # Package setup (editable install)    
â”‚â”€â”€ requirements.txt        # Dependencies    
â”‚â”€â”€ Dockerfile              # Containerization    
â”‚â”€â”€ test_mongodb.py         # Connectivity test for MongoDB    
â”‚â”€â”€ .github/workflows/      # CI/CD workflows (GitHub Actions)    
â”‚    
â”œâ”€â”€ Artifacts/              # Auto-generated run artifacts (timestamped)    
â”‚   â”œâ”€â”€ data_ingestion/     # feature_store/, ingested/{train.csv,test.csv}    
â”‚   â”œâ”€â”€ data_validation/    # drift_report/report.yaml    
â”‚   â”œâ”€â”€ data_transformation/# transformed/{train.npy,test.npy}, preprocessing.pkl    
â”‚   â”œâ”€â”€ model_trainer/      # trained_model/model.pkl    
â”‚   â”œâ”€â”€ model_evaluation/   # evaluation.yaml    
â”‚   â””â”€â”€ model_pusher/       # final packaging for serving    
â”‚    
â”œâ”€â”€ data_schema/            # (Optional) schema files for validation    
â”œâ”€â”€ final_model/            # final model + preprocessor for API serving    
â”œâ”€â”€ prediction_output/      # batch prediction outputs (if used)    
â”œâ”€â”€ templates/              # Web templates (if UI routes are added)    
â”œâ”€â”€ valid_data/             # curated/validated CSVs for quick tests    
â””â”€â”€ networksecurity/        # Core ML pipeline (see below) 
```

---

## ğŸ§  Core ML Pipeline â€“ `networksecurity/`

```
networksecurity/
â”œâ”€â”€ cloud/
â”‚   â””â”€â”€ s3_syncer.py           # Sync local folders with S3
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ data_ingestion.py      # Pull from MongoDB, split train/test
â”‚   â”œâ”€â”€ data_validation.py     # Schema checks, nulls, drift detection
â”‚   â”œâ”€â”€ data_transformation.py # Preprocessing, imputation
â”‚   â””â”€â”€ model_trainer.py       # Train model, log via MLflow
â”‚
â”œâ”€â”€ constant/
â”‚   â””â”€â”€ training_pipeline/__init__.py
â”‚       # Configs: TARGET_COLUMN, PIPELINE_NAME, ARTIFACT_DIR, FILE_NAME
â”‚
â”œâ”€â”€ entity/
â”‚   â”œâ”€â”€ config_entity.py       # Config objects
â”‚   â””â”€â”€ artifact_entity.py     # Pipeline artifact objects
â”‚
â”œâ”€â”€ exception/
â”‚   â””â”€â”€ exception.py           # Custom exception handling
â”‚
â”œâ”€â”€ logging/
â”‚   â””â”€â”€ logger.py              # Logging utility
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ training_pipeline.py   # Orchestrates all steps
â”‚   â””â”€â”€ batch_prediction.py    # Batch prediction runner
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ main_utils/utils.py    # Save/load objects, metrics, YAML utils
    â””â”€â”€ ml_utils/
        â”œâ”€â”€ metric/classification_metric.py
        â””â”€â”€ model/estimator.py # Preprocessor + model wrapper
```

---

## ğŸ”‘ Environment & Secrets(.env / GitHub Secrets)

Local `.env` file (create in repo root):    

MONGO_DB_URL="YOUR_MONGODB_URI"    
MONGODB_URL_KEY="YOUR_MONGODB_URI"   # used in app.py    
    
MLFLOW_TRACKING_URI="https://dagshub.com/<username>/<repo>.mlflow"    
MLFLOW_TRACKING_USERNAME="<your_username>"    
MLFLOW_TRACKING_PASSWORD="<your_token>"    
    
AWS_ACCESS_KEY_ID="<your_aws_access_key>"    
AWS_SECRET_ACCESS_KEY="<your_aws_secret_key>"    
AWS_DEFAULT_REGION="us-east-1"    
    
GitHub â†’ Settings â†’ Secrets â†’ Actions:    
    
AWS_ACCESS_KEY_ID    
AWS_SECRET_ACCESS_KEY    
AWS_REGION = us-east-1        
AWS_ECR_LOGIN_URI = 788614365622.dkr.ecr.us-east-1.amazonaws.com/networkssecurity    
ECR_REPOSITORY_NAME = networkssecurity    
MLFLOW_TRACKING_URI    
MLFLOW_TRACKING_USERNAME    
MLFLOW_TRACKING_PASSWORD    
MONGO_DB_URL   # or MONGODB_URL_KEY 

---

## ğŸ§­ Config Changes Before Run    

Open `networksecurity/constant/training_pipeline/__init__.py` and review:    

- TARGET_COLUMN = "Result"    
- FILE_NAME = "phisingData.csv"    
- ARTIFACT_DIR = "Artifacts"    
- MODEL_TRAINER_EXPECTED_SCORE = 0.60    
- TRAINING_BUCKET_NAME = "your-s3-bucket-name"    
    
---   
     
## âš¡ Quick Start (Local Setup)

ğŸ“‚ Clone Repository
```
git clone https://github.com/SK1240/Network-security.git    
cd Network-Security 
```   
    
### ğŸ› ï¸ Create Virtual Environment  
```
python -m venv .venv    
.venv\Scripts\activate     # Windows    
source .venv/bin/activate  # Linux/Mac   
``` 
    
### ğŸ“¦ Install Dependencies  
```
pip install -r requirements.txt    
```

### ğŸ”‘ Add Secrets   
```
nano .env    
```

---    
   
## ğŸ—„ï¸ MongoDB â€“ Load Data
### âœ… Test Connection
```
python test_mongodb.py
```

### ğŸ“¤ Push Data to MongoDB
```
python push_data.py
```

## ğŸ¤– Run Training Pipeline
### â–¶ï¸ Execute End-to-End Pipeline
```
python main.py
```

---

## ğŸ“‚ Artifacts will be created inside Artifacts/<timestamp>/:
* ğŸ“¥ data_ingestion/

* ğŸ§ data_validation/

* ğŸ”„ data_transformation/

* ğŸ‹ï¸ model_trainer/

* ğŸ“Š model_evaluation/

* ğŸ“¦ model_pusher/

---

## ğŸš€ Run API (FastAPI)
### â–¶ï¸ Start API Server
```
uvicorn app:app --host 0.0.0.0 --port 5000 --reload
```

### ğŸ—ï¸ Train via API
```
curl -X GET http://127.0.0.1:5000/train
```

### ğŸ” Predict via API (Upload CSV)
curl -X POST "http://127.0.0.1:5000/predict" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@valid_data/sample.csv"

---

## ğŸ“Š MLflow Experiments
### ğŸ–¥ï¸ Run MLflow Locally
```
mlflow ui --host 0.0.0.0 --port 5001
```
â¡ï¸ Open http://127.0.0.1:5001 in your browser

For DagsHub MLflow â†’ set `MLFLOW_TRACKING_URI`, `USERNAME`, `PASSWORD` in `.env`

---

## â˜ï¸ AWS Deployment (EC2 + Docker + ECR)
### ğŸ”‘ 1. Connect to EC2 Instance
Replace <EC2-IP> with your **EC2 public IPv4 address**, and `your-key.pem` with your private key file:
```
chmod 400 your-key.pem   # set proper permissions first
ssh -i "your-key.pem" ubuntu@<EC2-IP>
```

### ğŸ³ 2. Install Docker on EC2
```
sudo apt-get update -y
sudo apt-get upgrade -y
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker ubuntu
newgrp docker
docker --version
``` 

### ğŸ”§ 3. Configure AWS CLI
```
aws configure
```
ğŸ‘‰ Enter your **AWS Access Key**, **Secret Key**, **Region** when prompted.


### ğŸ·ï¸ 4. Login to Amazon ECR
AWS_REGION=us-east-1
AWS_ECR_LOGIN_URI=788614365622.dkr.ecr.$AWS_REGION.amazonaws.com
ECR_REPOSITORY_NAME=networkssecurity

aws ecr get-login-password --region $AWS_REGION \
| docker login --username AWS --password-stdin $AWS_ECR_LOGIN_URI


### ğŸ“¦ 5. Build & Push Docker Image
```
docker build -t $ECR_REPOSITORY_NAME:latest .
docker tag $ECR_REPOSITORY_NAME:latest $AWS_ECR_LOGIN_URI/$ECR_REPOSITORY_NAME:latest
docker push $AWS_ECR_LOGIN_URI/$ECR_REPOSITORY_NAME:latest
```
# ğŸ›¡ï¸Network Security - Phishing URL Detection Pipeline (End-to-End ML & MLOps)
This project demonstrates a complete **Machine Learning + MLOps workflow** for detecting phishing URLs. It covers **data ingestion â†’ validation â†’ transformation â†’ model training â†’ evaluation â†’ deployment**, with **MLflow tracking**, **Docker containerization**, **CI/CD** via GitHub Actions, and deployment on **AWS (EC2 + ECR + S3)**.

---

## âœ… Key Features

- Structured, production-style **pipeline** with **logging**, **exceptions**, and **artifacts**
- **ETL Pipeline**: Extract data from **MongoDB**, transform it **(imputation, preprocessing, feature engineering)**, and load it into the pipeline for model training
- **FastAPI** service for training and prediction (/train, /predict)
- **MLflow** integration for experiments (parameters, metrics, and artifacts)
- **Containerized with Docker** for easy deployment
- **AWS deployment** workflow: **EC2**, **ECR**, and **S3 storage**
- GitHub Actions workflows for **CI/CD** included
- Utilities to sync artifacts to/from **S3**

---

## ğŸ“¦ Project Structure

```
Network-Security/    
â”‚â”€â”€ app.py                  # FastAPI app: /train and /predict    
â”‚â”€â”€ main.py                 # Local CLI training entry (stage-by-stage)    
â”‚â”€â”€ push_data.py            # Ingest source data into MongoDB    
â”‚â”€â”€ setup.py                # Package setup (editable install)    
â”‚â”€â”€ requirements.txt        # Dependencies    
â”‚â”€â”€ Dockerfile              # Containerization    
â”‚â”€â”€ test_mongodb.py         # Connectivity test for MongoDB    
â”‚â”€â”€ .github/workflows/      # CI/CD workflows (GitHub Actions)    
â”‚    
â”œâ”€â”€ Artifacts/              # Auto-generated run artifacts (timestamped)    
â”‚   â”œâ”€â”€ data_ingestion/     # feature_store/, ingested/{train.csv,test.csv}    
â”‚   â”œâ”€â”€ data_validation/    # drift_report/report.yaml    
â”‚   â”œâ”€â”€ data_transformation/# transformed/{train.npy,test.npy}, preprocessing.pkl    
â”‚   â”œâ”€â”€ model_trainer/      # trained_model/model.pkl    
â”‚   â”œâ”€â”€ model_evaluation/   # evaluation.yaml    
â”‚   â””â”€â”€ model_pusher/       # final packaging for serving    
â”‚    
â”œâ”€â”€ data_schema/            # (Optional) schema files for validation    
â”œâ”€â”€ final_model/            # final model + preprocessor for API serving    
â”œâ”€â”€ prediction_output/      # batch prediction outputs (if used)    
â”œâ”€â”€ templates/              # Web templates (if UI routes are added)    
â”œâ”€â”€ valid_data/             # curated/validated CSVs for quick tests    
â””â”€â”€ networksecurity/        # Core ML pipeline (see below) 
```

---

## ğŸ§  Core ML Pipeline â€“ `networksecurity/`

```
networksecurity/
â”œâ”€â”€ cloud/
â”‚   â””â”€â”€ s3_syncer.py           # Sync local folders with S3
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ data_ingestion.py      # Pull from MongoDB, split train/test
â”‚   â”œâ”€â”€ data_validation.py     # Schema checks, nulls, drift detection
â”‚   â”œâ”€â”€ data_transformation.py # Preprocessing, imputation
â”‚   â””â”€â”€ model_trainer.py       # Train model, log via MLflow
â”‚
â”œâ”€â”€ constant/
â”‚   â””â”€â”€ training_pipeline/__init__.py
â”‚       # Configs: TARGET_COLUMN, PIPELINE_NAME, ARTIFACT_DIR, FILE_NAME
â”‚
â”œâ”€â”€ entity/
â”‚   â”œâ”€â”€ config_entity.py       # Config objects
â”‚   â””â”€â”€ artifact_entity.py     # Pipeline artifact objects
â”‚
â”œâ”€â”€ exception/
â”‚   â””â”€â”€ exception.py           # Custom exception handling
â”‚
â”œâ”€â”€ logging/
â”‚   â””â”€â”€ logger.py              # Logging utility
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ training_pipeline.py   # Orchestrates all steps
â”‚   â””â”€â”€ batch_prediction.py    # Batch prediction runner
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ main_utils/utils.py    # Save/load objects, metrics, YAML utils
    â””â”€â”€ ml_utils/
        â”œâ”€â”€ metric/classification_metric.py
        â””â”€â”€ model/estimator.py # Preprocessor + model wrapper
```